本篇博客主要详细介绍朴素贝叶斯模型。首先贝叶斯分类器是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类器。而朴素贝叶斯分类器是贝叶斯分类器中最简单，也是最常见的一种分类方法。并且，朴素贝叶斯算法仍然是流行的十大挖掘算法之一，该算法是有监督的学习算法，解决的是分类问题。该算法的优点在于简单易懂、学习效率高、在某些领域的分类问题中能够与决策树、神经网络相媲美。但由于该算法以自变量之间的独立（条件特征独立）性和连续变量的正态性假设为前提(这个假设在实际应用中往往是不成立的)，就会导致算法精度在某种程度上受影响。

朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法，是经典的机器学习算法之一。最为广泛的两种分类模型是决策树(Decision Tree Model)和朴素贝叶斯模型（Naive Bayesian Model，NBM）。和决策树模型相比，朴素贝叶斯分类器(Naive Bayes Classifier 或 NBC)发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。同时，NBC模型所需估计的参数很少，对缺失数据不太敏感，算法也比较简单。理论上，NBC模型与其他分类方法相比具有最小的误差率。

历史背景解读：

18世纪英国数学家托马斯·贝叶斯(Thomas Bayes，1702～1761)提出过一种看似显而易见的观点：“用客观的新信息更新我们最初关于某个事物的信念后，我们就会得到一个新的、改进了的信念。”这个研究成果由于简单显得平淡无奇，直至他死后两年才于1763年由他的朋友理查德·普莱斯帮助发表。它的数学原理很容易理解，简单说就是，如果你看到一个人总是做一些好事，则会推断那个人多半会是一个好人。这就是说，当你不能准确知悉一个事物的本质时，你可以依靠与事物特定本质相关的事件出现的多少去判断其本质属性的概率。用数学语言表达就是：支持某项属性的事件发生得愈多，则该属性成立的可能性就愈大。与其他统计学方法不同，贝叶斯方法建立在主观判断的基础上，你可以先估计一个值，然后根据客观事实不断修正。

1774年，法国数学家皮埃尔-西蒙·拉普拉斯(Pierre-Simon Laplace，1749-1827)独立地再次发现了贝叶斯公式。拉普拉斯关心的问题是：当存在着大量数据，但数据又可能有各种各样的错误和遗漏的时候，我们如何才能从中找到真实的规律。拉普拉斯研究了男孩和女孩的生育比例。有人观察到，似乎男孩的出生数量比女孩更高。这一假说到底成立不成立呢? 拉普拉斯不断地搜集新增的出生记录，并用之推断原有的概率是否准确。每一个新的记录都减少了不确定性的范围。拉普拉斯给出了我们现在所用的贝叶斯公式的表达：

   

该公式表示在B事件发生的条件下A事件发生的条件概率，等于A事件发生条件下B事件发生的条件概率乘以A事件的概率，再除以B事件发生的概率。公式中，P(A)也叫做先验概率，P(A/B)叫做后验概率。严格地讲，贝叶斯公式至少应被称为“贝叶斯-拉普拉斯公式”。

贝叶斯学派很古老，但是从诞生到一百年前一直不是主流。主流是频率学派。频率学派的权威皮尔逊和费歇尔都对贝叶斯学派不屑一顾，但是贝叶斯学派硬是凭借在现代特定领域的出色应用表现为自己赢得了半壁江山。贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类。

概率基础：

条件概率是指事件A在另外一个事件B已经发生条件下的发生概率。 条件概率表示为： P(A|B)， 读作“在B条件下A的概率”。若只有两个事件A， B， 那么：

​

 

 

 

概念：

先验概率：是指根据以往经验和分析得到的概率。例如如果我们对西瓜的色泽、根蒂和纹理等特征一无所知，按照常理来说，西瓜是好瓜的概率是60%。那么这个概率P（好瓜）就被称为先验概率。

后验概率：事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小。例如假如我们了解到判断西瓜是否好瓜的一个指标是纹理。一般来说，纹理清晰的西瓜是好瓜的概率大一些，大概是75%。如果把纹理清晰当作一种结果，然后去推测好瓜的概率，那么这个概率P（好瓜|纹理清晰）就被称为后验概率。后验概率类似于条件概率（纹理清晰的条件下是好瓜的概率）。

联合概率：设二维离散型随机变量（X，Y）所有可能取得值为，记则称 为随机变量X和Y的联合概率。计算如下：



例：在买西瓜的案例中，P(好瓜，纹理清晰)称为联合分布，它表示纹理清晰且是好瓜的概率。关于它的联合概率，满足以下乘法等式：



其中，P(好瓜|纹理清晰)就是后验概率，表示在“纹理清晰”的条件下，是“好瓜”的概率。P(纹理清晰|好瓜)表示在“好瓜”的情况下，是“纹理清晰”的概率。

 

贝叶斯定理：

 （注：P(x，c)=P(x│c)P(c)）



 

 

 



后验概率P(c∣x)，在现实任务中通常难以直接获得。所要实现的是基于有限的训练样本集尽可能准确地估计出后验概率P(c│x)。有两种策略:直接建模P(c∣x)来预测c,“判别式模型”；对联合概率分布P(x,c)P(x,c)P(x,c)建模，然后再由此获得P(c|x),“生成式模型”。Fisher判别式、支持向量机等,都可归入判别式模型的范畴。



对生成式模型（贝叶斯分类器）：对于每个特征x，我们想要知道样本在这个特性x下属于哪个类别，即求后验概率P(c|x)最大的类标记。这样基于贝叶斯公式，可以得到：



P(c)是类“先验”概率，P(x|c)是样本x相对于类标记c的类条件概率，或称似然。p(x)是用于归一化的“证据”因子，对于给定样本x，证据因子p(x)与类标记无关，即在下面的示例中分母不做处理。于是，估计p(c|x)的问题变为基于训练数据来估计先验P(c)和似然P(x∣c)，对于条件概率p(x|c)来说，它涉及x所有属性的联合概率。P(c)可通过各类样本出现的频率来进行估计。

 

逆概：（考虑事件）

贝叶斯分类器就是一种分类的方法，而且是一种基于贝叶斯原理，对联合概率分布p(x,c)建模，之后由条件概率公式得出后验概率的生成式模型的方法 。因为后验概率P(c∣x)，在现实任务中通常难以直接获得，所以朴素贝叶斯可以理解为求 "逆概" 。

例如：一座别墅在过去的 20 年里一共发生过 2 次被盗，别墅的主人有一条狗，狗平均每周晚上叫 3 次，在盗贼入侵时狗叫的概率被估计为 0.9，问题是：在狗叫的时候发生入侵的概率是多少？

假设 A 事件为狗在晚上叫，B 为盗贼入侵。

事件A（狗叫）：P(A) = 3/7

事件B（被偷）：P(B) = 2/(365*20+4)

盗贼入侵时狗叫的概率被估计为 0.9（先偷后叫）：P(A|B) = 0.9

考虑事件（反过来，逆概）：P(B|A) = P(A|B) * P(B) / P(A) = 0.9* (2/(365*20+4)) / (3/7)  = 0.000575

 

极大似然估计：

估计类条件概率的一种常用策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。

假设P(x∣c)具有确定的形式并且被参数向量θc唯一确定，则我们的任务就是利用训练集D估计参数θc，我们将P(x|c)记为P(x∣θc)

参数估计有两个学派：

1）频率主义学派：认为参数虽然未知,但却是客观存在的固定值,可通过优化似然函数等准则来确定参数值。经典的方法：极大似然估计。

2）贝叶斯学派：认为参数是未观察到的随机变量, 可假定参数服从一个先验分布,然后基于观测到的数据来计算参数的后验分布。

Dc表示训练集D中第c类样本组成的集合,假设这些样本是独立同分布的，则参数θc对于数据集Dc的似然是：



同时发生，因此连乘，对θc进行极大似然估计,就是去寻找能最大化似然P(Dc∣θc)的参数值θc，直观上看，极大似然估计是试图在θc所有可能的取值中，找到一个能使数据出现的“可能性”最大的值。

上式的连乘操作易造成下溢，通常使用对数似然：



连乘转换为连加，更好处理。此时参数θc 的极大似然估计为：



在连续属性情形下，假设概率密度函数，则参数和的极大似然估计为：

 



似然函数 L(x；θ)L(x；θ)L(x；θ)在形式上，其实就是样本的联合密度。把x1，x2，x3，…，xn看作常数，而把待定参数θ0，θ1，…，θn看作 L 的自变量。对连续型总体X 和离散型随机变量X，样本的似然函数分别是概率密度和分布率的连乘形式。

 

朴素贝叶斯算法：

是在贝叶斯算法的基础上进行了相应的简化，即假定给定目标值时属性之间相互条件独立。也就是说没有哪个属性变量对于决策结果来说占有着较大的比重，也没有哪个属性变量对于决策结果占有着较小的比重。虽然这个简化方式在一定程度上降低了贝叶斯分类算法的分类效果，但是在实际的应用场景中，极大地简化了贝叶斯方法的复杂性。在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数Y=f(X),要么是条件分布P(Y|X)。但是朴素贝叶斯却是生成方法，也就是直接找出特征输出Y和特征X的联合分布P(X,Y),然后用P(Y|X)=P(X,Y)/P(X)得出。

基于贝叶斯公式来估计后验概率P(c∣x)的主要困难：类条件概率P(x∣c)是所有属性上的联合概率，难以从有限的训练样本直接估计而得。朴素贝叶斯分类器采用了“属性条件独立性假设”：假设所有属性相互独立。基于属性条件独立性假设,贝叶斯公式可重写为：



其中，d为属性数目，xi为x在第i个属性上的取值。由于对于所有的类别p(x)相同，即对于同一个样本而言P(x)都是一样的，对分类结果没有什么影响，为了提高运行效率，我们在计算时可以不考虑证据因子P(x)。最大化后验概率便转化为了最大化似然与先验的乘积。基于上式的贝叶斯判定准则有：

 

这就是朴素贝叶斯分类器的表达式。朴素贝叶斯分类器的训练过程就是基于训练集D来估计类先验概率P(c),并为每个属性估计条件概率P(xi∣c)

类先验概率：若Dc表示训练集D中第c类样本组成的集合，若有充足的独立同分布样本，则可容易的估计出来类先验概率



 对于离散属性而言，Dc,xi表示第c类第i个属性上取值为xi的样本，则条件概率P(xi∣c)可估计为：

c



对于连续属性可考虑为概率密度函数，假定  分别是第c类样本在第i个属性上取值的均值和方差，则有：



对于朴素贝叶斯还需要说明的一点是：若某种属性值在训练集中没有与某个类同时出现过，则直接基于概率估计公式得出来概率为0，再通过各个属性概率连乘式计算出的概率也为0，这就导致没有办法进行分类了，所以，为了避免属性携带的信息被训练集未出现的属性值“抹去”，在估计概率值时通常要进行“平滑”，常用“拉普拉斯修正”，具体来讲，令N表示训练集D中可能的类别数，Ni表示第i个属性可能的取值数。



 

拉普拉斯修正避免了样本不充分而导致概率估计为零的问题，并且在训练集样本变大的时候，修正过程所引入的先验的影响也会逐渐变得可忽视，使得估计值越来越接近实际概率值。

 

案例分析1：

给定如下数据：



根据以上数据，现在有一对男女朋友，男生向女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请判断女生是嫁还是不嫁?

该问题转换为数学问题就是比较，该问题转换为数学问题就是比较：P( 嫁 | 不帅，性格不好，身高矮，不上进 ) 与 P( 不嫁 | 不帅，性格不好，身高矮，不上进 ) 的概率。

由贝叶斯公式得：



 假设各个特征相互独立，即：



 或者（其实没区别）：



首先我们整理训练数据中：嫁的样本数总共有6个，则 P（嫁） = 6 / 12 = 1 / 2；不帅，也嫁了的样本数总共有6个，则 P（不帅 | 嫁） = 3 / 6 = 1 / 2；   性格不好，也嫁了的样本数总共有1个，则 P（性格不好 | 嫁） = 1 / 6  ；身高矮，也嫁了的样本数总共有1个，则 P（身高矮 | 嫁） = 1 / 6 ；不上进，也嫁了的样本数总共有1个，则 P（不上进 | 嫁） = 1 / 6 ；

 

同理：

不嫁的样本数总共有6个，则 P（不嫁） = 6 / 12 = 1 / 2；不帅，就不嫁的样本数总共有1个，则 P（不帅 | 不嫁） = 1 / 6；性格不好，就不嫁的样本数总共有3个，则P（性格不好 | 不嫁） = 3 / 6 = 1 / 2；   身高矮，就不嫁的样本数总共有6个，则P（身高矮 | 不嫁） = 6 / 6 = 1；不上进，就不嫁的样本数总共有3个，则P（不上进 | 不嫁） = 3 / 6 = 1 / 2；由于分母都相同，且分子(1 / 864) < (1 / 48) ，所以最后得出的结论是该女生不嫁给这个男生。

到此应该能整明白贝叶斯算法是原理了，如果还没整明白，没关系，再来看一个例子。

案例解析2：

用西瓜数据集训练一个朴素贝叶斯分类器，进行分类：



对下表示例进行预测：

 

 1. 求出先验概率P(c)：



 2. 求出每个属性估计条件概率P(xi∣c)：

     离散属性：



     连续属性：



3. 最大化后验概率，即计算先验与类条件概率的乘积，数值大的对应的分类为分类结果。即采用朴素贝叶斯分类器的方法，假设各属性之间相互独立；不考虑证据因子P(x)，转化为: P(c∣x)=P(x∣c)P(c)；最大化后验概率便转化为了最大化似然与先验的乘积，maxP(x∣c)P(c)转化为：



 

 

4. 预测结果，由计算结果可知，由于0.038>6.80×10−5，即P(好瓜=是)>P(好瓜=否)，此瓜更有可能是好瓜，因此朴素贝叶斯分类器将测试样本“测1”判别为“好瓜”。

5. 拉普拉斯修（补充）



 

朴素贝叶斯算法实现原理总结：

贝叶斯分类器目的就是分类，即判断含有属性x的样本属于哪一类，也就是判断后验P(c│x)在不同的类别时概率的大小，后验概率越大说明所属的类别越有可能是正确的类别。因此，我们的目标就转化为了最大化后验概率，然后将后验概率最大的类别判定为该样本所属的类别。例如：maxP(c∣x)=P(c2∣x)则x属于c2。对于同一个样本而言P(x)都是一样的，对分类结果没有什么影响，为了提高运行效率，我们在计算时可以不考虑证据因子P(x)。最大化后验概率便转化为了最大化似然与先验的乘积。则P(c∣x)=P(x∣c)P(c)/P(x)​ ,在不考虑P(x)后转化为: P(c∣x)=P(x∣c)P(c)。

 

朴素贝叶斯的优缺点：
优点：

1. 对小规模的数据表现很好，适合多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练；

2. 对缺失数据不太敏感，算法也比较简单，常用于文本分类；

3. 发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率，当数据呈现不同的特点时，分类性能不会有太大的差异，健壮性好；

4. 当数据集属性之间的关系相对比较独立时，朴素贝叶斯分类算法会有较好的效果。
缺点：

1. 对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）；

2. 需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳；

3. 由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率；

4. 理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型给定输出类别的情况下,假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。

 

补充：

1）半朴素贝叶斯分类器：（详细内容待总结）在朴素的分类中， 我们假定了各个属性之间的独立，这是为了计算方便，防止过多的属性之间的依赖导致的大量计算。这正是朴素的含义，虽然朴素贝叶斯的分类效果不错，但是属性之间毕竟是有关联的， 某个属性依赖于另外的属性， 于是就有了半朴素贝叶斯分类器。

 2）朴素贝叶斯与LR的区别？简单来说：朴素贝叶斯是生成模型，根据已有样本进行贝叶斯估计学习出先验概率P(Y)和条件概率P(X|Y)，进而求出联合分布概率P(XY),最后利用贝叶斯定理求解P(Y|X)， 而LR是判别模型，根据极大化对数似然函数直接求出条件概率P(Y|X)；朴素贝叶斯是基于很强的条件独立假设（在已知分类Y的条件下，各个特征变量取值是相互独立的），而LR则对此没有要求；朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。

3）在估计条件概率P(X|Y)时出现概率为0的情况怎么办？简单来说：引入λ，当λ=1时称为拉普拉斯平滑。

4）一句话概况朴素贝斯：是一个生成模型（很重要），其次它通过学习已知样本，计算出联合概率，再求条件概率。

5）生成模式和判别模式的区别：生成模式：由数据学得联合概率分布，求出条件概率分布P(Y|X)的预测模型；常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机。判别模式：由数据学得决策函数或条件概率分布作为预测模型；常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场。

 

 

 参考文章：

 1. https://blog.csdn.net/ch18328071580/article/details/94407134

 2. https://blog.csdn.net/yangjingjing9/article/details/79986371

 3. https://www.cnblogs.com/pinard/p/6069267.html

 4. https://blog.csdn.net/sinat_30353259/article/details/80932111
